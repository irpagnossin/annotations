{
 "metadata": {
  "name": "",
  "signature": "sha256:826d96f42d2afb876849d6b1d6121706160781a3bde58a495e572d5df38e4aac"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Redes neurais no formalismo matricial"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.display import display, Math, Latex\n",
      "import numpy as np"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Suponha que, para um certo problema, tenhamos $N_x$ par\u00e2metros de entrada e $N_y$ par\u00e2metros de sa\u00edda, organizados cada qual numa tupla, assim:\n",
      "$$\n",
      "x = \\left[\\begin{matrix}x_1\\\\x_2\\\\\\vdots\\\\x_{N_y}\\end{matrix}\\right]\\qquad\\text{e}\\qquad\n",
      "y = \\left[\\begin{matrix}y_1\\\\y_2\\\\\\vdots\\\\y_{N_y}\\end{matrix}\\right],\n",
      "$$\n",
      "\n",
      "Se para $M$ exemplos conhecemos tanto $x$ quanto $y$, podemos construir uma rede neural que, \"observando\" a correla\u00e7\u00e3o entre $x$ e $y$, pode reproduz\u00ed-la com boas chances de acerto para valores $x$ desconhecidos.\n",
      "\n",
      "Esse conjunto de pares $(x,y)$ \u00e9 o **conjunto de treinamento** da rede neural e \u00e9 representado pelas matrizes $X$ e $Y$:\n",
      " \n",
      "$$X = \\left[\\matrix{\n",
      "| & | & & | \\\\\n",
      "x^1 & x^2 & \\cdots & x^M \\\\\n",
      "| & | & & |\n",
      "}\\right]\\qquad\\text{e}\\qquad\n",
      "Y = \\left[\\matrix{\n",
      "| & | & & | \\\\\n",
      "y^1 & y^2 & \\cdots & y^M \\\\\n",
      "| & | & & |\n",
      "}\\right]$$\n",
      "\n",
      "\n",
      "A estrutura dessa rede neural pode ser facilmente representada por um vetor que contenha a quantidade de unidades em cada camada. Por exemplo,"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "U = np.array([2, 3, 1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "representa uma rede neural com $L = 3$ camadas (tamanho do vetor $s$): a primeira camada --- a de entrada --- cont\u00e9m 2 unidades de entrada; a segunda cont\u00e9m 3 e, finalmente, a terceira (sa\u00edda) cont\u00e9m 1. E se chamarmos $U_\\ell$ a quantidade de unidades que pertencem \u00e0 camada $\\ell \\in \\{1,2,\\ldots,L\\}$, ent\u00e3o $U_1 = 2$, $U_2 = 3$ e $U_3 = 1$.\n",
      "\n",
      "Note ainda que sempre valem as rela\u00e7\u00f5es $N_x = U_1$ e $N_y = U_L$. O motivo \u00e9 simples: queremos introduzir os $N_x$ par\u00e2metros de $x$ na primeira camada da rede. Para isso pecisamos de $U_1 = N_x$ unidades de entrada. Analogamente, se queremos obter $N_y$ par\u00e2metros de sa\u00edda, a \u00faltima camada da rede neural deve conter $U_L = N_y$ unidades."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "L = len(U)\n",
      "print \"Quantidade de camadas da rede neural: \" + str(L)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Quantidade de camadas da rede neural: 3\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Preparando o terreno"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Neste cen\u00e1rio, o problema que a rede neural deve resolver \u00e9 encontrar $L-1$ matrizes de par\u00e2metros, uma para cada camada, desde a primeira at\u00e9 a pen\u00faltima: $\\Theta^\\ell$. Suas dimens\u00f5es s\u00e3o: $U_{\\ell+1}\\times\\left(U_\\ell+1\\right)$.\n",
      "\n",
      "Assim, no nosso exemplo teremos 2 matrizes $\\Theta$:\n",
      "* $\\Theta^1 \\equiv []_{U_2\\times (U_1+ 1)} = []_{3\\times 3}$\n",
      "* $\\Theta^2 \\equiv []_{U_3\\times (U_2+1)} = []_{1\\times 4}$\n",
      "\n",
      "Ocorre que $\\Theta_1$ \u00e9 a matriz que, atuando sobre $x$, transforma-o num outro vetor, conforme essa transforma\u00e7\u00e3o:\n",
      "\n",
      "Chame $u_\\ell$ de a unidade $u$ da camada $\\ell$. Ent\u00e3o, tomemos, para come\u00e7ar, a unidade 1 da camada 2: $u_2 = 1$. Essa unidade transforma $x$ da seguinte maneira:\n",
      "$$\n",
      "z^2_1 = \\Theta^1_{10} + \\Theta^1_{11} x^1 + \\Theta^{1}_{12} x^2 = \\left[\\begin{matrix}\\Theta^1_{10} &\\Theta^1_{11} & \\Theta^{1}_{12}\\end{matrix}\\right] \\cdot \\left[\\begin{matrix}1\\\\x^1\\\\x^2\\\\\\end{matrix}\\right] = \\Theta^\\ell b^\\ell\n",
      "$$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Passo X: criamos as matrizes $\\Theta$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Criamos um vetor de matrizes $\\Theta$ de comprimento $L - 1$. Visando otimizar o funcionamento do algoritmo, precisamos criar essas matrizes com valores n\u00e3o nulos. Para isso, costuma-se popular as matrizes com valores aleat\u00f3rios, que variam um intervalo $[-\\epsilon_\\ell,+\\epsilon_\\ell]$, sendo que $\\epsilon_\\ell = \\sqrt{6/(u_{l} + u_{l+1})}$, com $\\ell \\in \\{0, 1, \\ldots, L-1\\}$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from numpy.random import random_sample\n",
      "import math\n",
      "\n",
      "thetas = []\n",
      "for l in np.arange(0,L-1):\n",
      "    epsilon = math.sqrt(6/(U[l]+U[l+1])) # Regra de ouro. N\u00e3o sei de onde vem :(\n",
      "    thetas.append(-epsilon + 2 * epsilon * random_sample((U[l+1],U[l] + 1))) # Matriz aleat\u00f3ria de dimens\u00f5es U_{l+1} por (U_l + 1)\n",
      "    \n",
      "for l in np.arange(0, L-1):\n",
      "    print \"Theta_\" + str(l) + \" =\\n\\t\" + str(thetas[l])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Theta_0 =\n",
        "\t[[-0.82856198  0.43256782  0.96247389]\n",
        " [-0.17740253 -0.57116359  0.73538414]\n",
        " [-0.3052372  -0.96533675 -0.21247928]]\n",
        "Theta_1 =\n",
        "\t[[-0.30354345 -0.16517925 -0.6192052  -0.39271655]]\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Esses s\u00e3o os par\u00e2metros que realizam a m\u00e1gica da rede neural.\n",
      "\n",
      "Deste modo, definimos uma posi\u00e7\u00e3o arbitr\u00e1ria no espa\u00e7o hiperdimensional dos par\u00e2metros $\\Theta$. A partir daqui, nosso objetivo \u00e9 percorrer a superf\u00edcie $J(\\Theta^\\ell)$, uma para cada camada, procurando seu m\u00ednimo."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import operator\n",
      "n_params = reduce(operator.add, map(np.size, thetas))\n",
      "print \"Quantidade de par\u00e2metros que precisamos ajustar: \" + str(n_params)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Quantidade de par\u00e2metros que precisamos ajustar: 13\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Passo X: $X$ e $y$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Para este tutorial, vamos definir:\n",
      "\n",
      "$$\n",
      "f : \\mathcal{R}^2 \\to \\mathcal{R}\\\\\n",
      "y = f(x_1,x_2) = \\left(x_1 - 1/2\\right)^2 + (y - 1)^2\n",
      "$$\n",
      "\n",
      "Perceba que este \u00e9 um problema que se encaixa na rede neural $s$, que definimos anteriormente. Isto \u00e9, temos dois par\u00e2metros de entrada ($x_1$ e $x_2$) e um de sa\u00edda [$y \\equiv f(x_1,x_2)$]."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def f(x,y):\n",
      "    return (x - 0.5)**2 + (y - 1)**2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Mas a rede n\u00e3o conhece $f$. Tudo que ela receber\u00e1 \u00e9 um conjunto de pares $(x_1,x_2)$, cada qual associado a um $y$. Vamos criar esse conjunto por meio de uma malha de pontos no plano $x_1 \\times x_2$:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = np.transpose([[x,y] for x in range(-2,2) for y in range(-2,2)])\n",
      "print X"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[-2 -2 -2 -2 -1 -1 -1 -1  0  0  0  0  1  1  1  1]\n",
        " [-2 -1  0  1 -2 -1  0  1 -2 -1  0  1 -2 -1  0  1]]\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note que o conjunto de treinamento cont\u00e9m $M$ exemplos, isto \u00e9, temos $M$ ternos $(x_1,x_2,y)$ conhecidos. Usaremos eles para treinar a rede neural."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "M = X.shape[1]\n",
      "print \"M = \" + str(M)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "M = 16\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Agora, determinamos os $y$ associados:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def f(x,y):\n",
      "    return (x - 0.5)**2 + (y - 1)**2\n",
      "\n",
      "y = []\n",
      "for i in range(0,M):\n",
      "    x1, x2 = X[0,i], X[1,i]\n",
      "    y.append(f(x1,x2))\n",
      "    \n",
      "y = np.array(y)\n",
      "print y"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 15.25  10.25   7.25   6.25  11.25   6.25   3.25   2.25   9.25   4.25\n",
        "   1.25   0.25   9.25   4.25   1.25   0.25]\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Em tempo, perceba ainda que cada exemplo \u00e9 composto por um valor de entrada, composto por $N = 2 = U_1$ par\u00e2metros:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "N = X.shape[0]\n",
      "print \"N = \" + str(N)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "N = 2\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Al\u00e9m disso, cada valor de sa\u00edda \u00e9 composto por $U_L = 1$ par\u00e2metros (pois $y$ \u00e9 um vetor de n\u00fameros)."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "_Feedforward_"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\"Feedforward\" \u00e9 o procedimento para determinar o custo, ou erro, do ajuste caracterizado por $\\Theta$, ou seja, $J(\\Theta)$. Para isso, determine $a_\\ell$, $b_\\ell$ e $z_\\ell$ iterativamente, conforme as express\u00f5es abaixo:\n",
      "* $a^\\ell = \\begin{cases}X& \\text{se }\\ell = 1\\\\ g(z^\\ell) & \\text{se }\\ell > 1\\end{cases} \\equiv []_{U_\\ell\\times M}$\n",
      "* $b^\\ell = \\left[\\matrix{1\\\\a^\\ell}\\right] \\equiv []_{(U_\\ell+1)\\times M}$\n",
      "* $z^{\\ell+1} = \\Theta^{\\ell} b^{\\ell} \\equiv []_{U_\\ell\\times M}$\n",
      "\n",
      "Ap\u00f3s isso, determine $\\alpha = - Y \\odot \\log\\left[z^L\\right] - (1-Y) \\odot \\log\\left[1-z^L\\right] \\equiv []_{U_L\\times M}$ e, finalmente, $J(\\Theta) = \\frac{1}{M}\\text{sum}(\\alpha) + \\frac{\\lambda}{2M} \\textrm{sum}\\left(\\Theta_\\ell^2\\right)$.\n",
      "\n",
      "$\\odot$ \u00e9 o operador de multiplica\u00e7\u00e3o elemento-a-elemento e $\\text{sum}(\\alpha)$ soma todos os elementos de $\\alpha$. Note que, na express\u00e3o de $\\alpha$, $1$ \u00e9 a matriz identidade de dimens\u00f5es iguais \u00e0 de $Y$. $g(x) \\doteq 1/\\left(1 + e^{-x}\\right)$ \u00e9 a fun\u00e7\u00e3o \"sigmoid\"."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Come\u00e7ando com $\\ell = 0$, calculamos $a_0 = X$, uma matriz de dimens\u00f5es $U_0 \\times M$. Ela representa os par\u00e2metros que s\u00e3o inseridos na rede, isto \u00e9, que incidem na camada $\\ell = 0$. Em seguida, constru\u00edmos a matriz $b_0$, acrescentando uma linha de 1 no topo de $a_0$. Essa matriz, de dimens\u00e3o $(U_0 + 1)\\times M$, representa os par\u00e2metros que, emergindo da camada $\\ell = 0$, incidem na camada $\\ell = 1$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "_Back propagation_"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\"Back propagation\" \u00e9 o procedimento usado para determinar o gradiente de $J(\\Theta)$. Para isso, efetue os c\u00e1lculos abaixo iterativamente, da \u00faltima camada ($\\ell = L$) para a primeira ($\\ell = 1$)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* $\\delta^\\ell = \\begin{cases}z^L - Y & \\text{se } \\ell = L \\\\\\left(\\Xi^\\ell\\right)^T \\delta^{\\ell+1} \\odot g'(z^\\ell)& \\text{se }\\ell < L\\end{cases}$\n",
      "* $\\partial_{\\Theta_\\ell} J(\\Theta) = \\frac{1}{M}\\delta_{\\ell+1}\\left(b_\\ell\\right)^T + \\frac{\\lambda}{M}\\Xi_\\ell \\equiv []_{U_{\\ell+1}\\times(U_\\ell+1)}$\n",
      "\n",
      "para $\\ell = L, L-1, \\ldots, 2$ (n\u00e3o existe $\\delta^1$). $\\Xi^\\ell = \\Theta^\\ell(:,2:\\text{end})$ \u00e9 a pr\u00f3pria matriz $\\Theta^\\ell$, s\u00f3 que sem a primeira **coluna**."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$g'(x) \\equiv dg/dx = g(x) \\left[1 - g(x)\\right]$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Implementa\u00e7\u00e3o do feedforward e do back propagation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Fun\u00e7\u00e3o sigmoid e sua derivada:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Sigmoid\n",
      "def g(x):\n",
      "    return 1.0 / (1.0 + np.exp(-x))\n",
      "\n",
      "# Derivada da sigmoid\n",
      "def grad_g(x):\n",
      "    return g(x) * (1.0 - g(x))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Abaixo, a implementa\u00e7\u00e3o da fun\u00e7\u00e3o $J(\\Theta, X, y, \\lambda)$, que retorna o par $(J, \\partial_{\\Theta} J)$. $\\lambda$ \u00e9 o par\u00e2metro de regulariza\u00e7\u00e3o, cuja fun\u00e7\u00e3o \u00e9 priorizar ajustes com par\u00e2metros de menor ordem."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def stack_ones(a):\n",
      "    ones = np.ones(1 if a.size == a.shape[0] else a.shape[1]) # TODO: tem um jeito melhor?\n",
      "    return np.vstack((ones, a))\n",
      "    \n",
      "def J(thetas, X, y, lamb):\n",
      "    zs = [1] # Esse 1 s\u00f3 serve para preencher\n",
      "    bs = []\n",
      "    reg = 0\n",
      "    \n",
      "    # Determina a fun\u00e7\u00e3o de custo do ajuste caracterizado pelos par\u00e2metros thetas\n",
      "    for layer in range(0, L-1):\n",
      "        a = X if layer == 0 else g(zs[layer])\n",
      "        b = stack_ones(a)\n",
      "        bs.append(b)\n",
      "        z = np.dot(thetas[layer], b)\n",
      "        zs.append(z)\n",
      "        reg = reg + np.power(thetas[layer][:,1:], 2).sum() # Regulariza\u00e7\u00e3o de J\n",
      "    \n",
      "    reg = reg * lamb / (2 * M)\n",
      "    Z = g(zs[L-1])\n",
      "    J = -(y * np.log(Z) + (1-y) * np.log(1-Z)).sum() / M + reg\n",
      "    \n",
      "    # Determina o gradiente de thetas\n",
      "    ds = [0 for _ in range(0,L)]\n",
      "    grad_J = [0 for _ in range(0,L-1)]\n",
      "    \n",
      "    for layer in range(L-1, -1, -1):\n",
      "        \n",
      "        if layer == L-1:\n",
      "            ds[layer] = zs[layer] - y\n",
      "        else:\n",
      "            xsi = np.delete(thetas[layer], 0, 1).T\n",
      "            ds[layer] = np.dot(xsi, ds[layer+1]) * grad_g(zs[layer])\n",
      "                \n",
      "        if layer < L-1:\n",
      "            grad_J[layer] = np.dot(ds[layer+1], bs[layer].T) / M\n",
      "            grad_J[layer][:,1:] = grad_J[layer][:,1:] + lamb/M * thetas[layer][:,1:] # Regulariza\u00e7\u00e3o de grad_J\n",
      "            \n",
      "    return (J, grad_J)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.optimize import fmin_bfgs\n",
      "\n",
      "def cost_function(theta):\n",
      "    cost, grad = J(theta, X, y, 0)\n",
      "    return cost\n",
      "\n",
      "def grad_cost(theta):\n",
      "    cost, grad = J(theta, X, y, 0)\n",
      "    return grad.flatten\n",
      "\n",
      "ans = fmin_bfgs(cost_function, thetas, maxiter=100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Warning: Desired error not necessarily achieved due to precision loss.\n",
        "         Current function value: nan\n",
        "         Iterations: 1\n",
        "         Function evaluations: 168\n",
        "         Gradient evaluations: 42\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ans"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "array([ array([[ 1257.43187503,  1258.69300484,  1259.2229109 ],\n",
        "       [ 1258.08303448,  1257.68927342,  1258.99582115],\n",
        "       [ 1257.95519981,  1257.29510026,  1258.04795773]]),\n",
        "       array([[ 12200.99858057,  12201.13694478,  12200.68291882,  12200.90940747]])], dtype=object)"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}