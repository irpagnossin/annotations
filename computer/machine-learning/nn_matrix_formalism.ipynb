{
 "metadata": {
  "name": "",
  "signature": "sha256:f3c5a465d06b496f72b43102112552ba4d268244e1310a59803274bc69e57e4c"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Redes neurais no formalismo matricial"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.display import display, Math, Latex\n",
      "from matplotlib.pyplot import plot\n",
      "import numpy as np\n",
      "\n",
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 51
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Representa\u00e7\u00e3o dos exemplos e do conjunto de treinamento"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Suponha que um certo problema mapeie o par\u00e2metro $x$ no par\u00e2metro $y$, ambos conhecidos. O par $(x,y)$ comp\u00f5e o que chamamos de \"exemplo\", pois nossa rede neural o utilizar\u00e1 para aprender, empiricamente, a correla\u00e7\u00e3o entre $x$ e $y$. Esses par\u00e2metros podem ser representados por vetores coluna, de $N_x$ e $N_y$ componentes, respectivamente:\n",
      "$$\n",
      "x = \\left[\\begin{matrix}x_1\\\\x_2\\\\\\vdots\\\\x_{N_y}\\end{matrix}\\right]\\qquad\\text{e}\\qquad\n",
      "y = \\left[\\begin{matrix}y_1\\\\y_2\\\\\\vdots\\\\y_{N_y}\\end{matrix}\\right],\n",
      "$$\n",
      "\n",
      "Mas um exemplo n\u00e3o \u00e9 suficiente para descobrirmos a correla\u00e7\u00e3o entre $x$ e $y$. Precisamos de um **conjunto de treinamento**, ou seja, um agrupamento de $M$ exemplos: $\\{(x_i, y_i)\\}$, com $i = \\{1, 2, \\ldots, M\\}$. Esse conjunto tamb\u00e9m pode ser representado matricialmente, assim:\n",
      "\n",
      "$$X = \\left[\\matrix{\n",
      "| & | & & | \\\\\n",
      "x^1 & x^2 & \\cdots & x^M \\\\\n",
      "| & | & & |\n",
      "}\\right]\\qquad\\text{e}\\qquad\n",
      "Y = \\left[\\matrix{\n",
      "| & | & & | \\\\\n",
      "y^1 & y^2 & \\cdots & y^M \\\\\n",
      "| & | & & |\n",
      "}\\right]$$\n",
      "\n",
      "Aten\u00e7\u00e3o para a nomenclatura: $x$ e $y$ (min\u00fasculos) representam **um** exemplo, isto \u00e9, um elemento do conjunto de treinamento. Por outro lado, $X$ e $Y$ (mai\u00fasculos) representam todo o conjunto de treinamento, composto por $M$ desses exemplos."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Representa\u00e7\u00e3o da rede neural"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Uma rede neural pode ser univocamente representada por um vetor que contenha a quantidade de unidades em cada camada. Por exemplo,"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nns = np.array([2, 3, 1]) # neural network structure\n",
      "\n",
      "L = len(U)\n",
      "print \"Quantidade de camadas da rede neural: \" + str(L)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Quantidade de camadas da rede neural: 3\n"
       ]
      }
     ],
     "prompt_number": 45
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "representa uma rede neural com $L = 3$ camadas (tamanho do vetor <code>nns</code>): a primeira camada (entrada da rede neural) cont\u00e9m 2 unidades de entrada; a segunda cont\u00e9m 3 e, finalmente, a terceira (sa\u00edda) cont\u00e9m 1. E se chamarmos $U_\\ell$ a quantidade de unidades que pertencem \u00e0 camada $\\ell \\in \\{1,2,\\ldots,L\\}$, ent\u00e3o $U_1 = 2$, $U_2 = 3$ e $U_3 = 1$, neste exemplo.\n",
      "\n",
      "A primeira camada \u00e9 a que recebe as $N_x$ componentes de $x$, ou seja, \u00e9 a nossa porta de entrada para a rede neural. Por isso, $N_x = U_1$ sempre. Analogamente, a \u00faltima camada devolve $N_y$ componentes de $y$, de modo que $N_y = U_L$. Assim, depois que a rede neural estiver treinada, se quisermos descobrir qual \u00e9 o $y$ associado a um $x$ dado e _n\u00e3o necessariamente pertencente ao conjunto de treinamento_, basta inser\u00ed-lo na primeira camada da rede neural e coletar $y$ na sa\u00edda."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Como a rede neural determina $y$?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Na verdade, esta \u00e9 uma decis\u00e3o sua e a chamamos de [**heur\u00edstica**](http://www.aulete.com.br/heuristica) (cf. o Aulete, \"conjunto de regras e m\u00e9todos para chegar-se \u00e0 inven\u00e7\u00e3o, \u00e0 descoberta ou \u00e0 resolu\u00e7\u00e3o de problemas\"). Outro nome \u00e9 **hip\u00f3tese**. Dito de outra forma, ningu\u00e9m sabe, mas voc\u00ea tem a liberdade de experimentar.\n",
      "\n",
      "Neste exemplo, n\u00f3s usaremos uma heur\u00edstica bastante difundida chamada **regress\u00e3o log\u00edstica**, que caracteriza-se pelo fato de que $y$ s\u00f3 pode assumir os valores zero ou um. Matematicamente, $y \\in \\{0,1\\}$ para qualquer $x$ do dom\u00ednio. Isso pode parcer limitado, mas considere o exemplo de determinar se uma pessoa tem c\u00e2ncer ou n\u00e3o a partir dos resultados de v\u00e1rios exames que ela fez. Neste cen\u00e1rio, $x_i$ seria o resultado do $i$-\u00e9simo exame, $y = 1$ representaria \"sim, ela tem c\u00e2ncer\" e $y = 0$ representaria \"n\u00e3o, ela n\u00e3o tem c\u00e2ncer\".\n",
      "\n",
      "Mas como temos muito mais ferramentas matem\u00e1ticas para lidar com problemas de vari\u00e1veis cont\u00ednuas, \n",
      "\n",
      "Para resolver esse tipo de problema, utilizamos a fun\u00e7\u00e3o [sigmoid](http://mathworld.wolfram.com/SigmoidFunction.html):\n",
      "$$\n",
      "g(x) \\doteq \\frac{1}{1+e^{-x}}.\n",
      "$$\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Em Python fica assim:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Sigmoid\n",
      "def g(x):\n",
      "    return 1.0 / (1.0 + np.exp(-x))\n",
      "\n",
      "# Derivada da sigmoid\n",
      "def grad_g(x):\n",
      "    return g(x) * (1.0 - g(x))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 46
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note que aproveitamos para definir tamb\u00e9m sua derivada, $\\partial_x g(x)$, pois precisaremos dela mais \u00e0 frente. Perceba que preferimos utilizar o operador nabla porque $x$ \u00e9, em geral, um vetor. Ou seja, $g(x)$ \u00e9, de fato, uma fun\u00e7\u00e3o de muitas vari\u00e1veis ($N_x$ vari\u00e1veis, para ser mais exato).\n",
      "\n",
      "O gr\u00e1fico de $g(x)$ est\u00e1 desenhado abaixo:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "range = np.arange(-10,10,0.1)\n",
      "plot(range, [g(x) for x in range])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 53,
       "text": [
        "[<matplotlib.lines.Line2D at 0x7f73e501d890>]"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAXMAAAD/CAYAAAADvzaFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkiRJkiRJkqQI/hdH5IJA/vEoPAAAAABJRU5ErkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0x7f73e51d8150>"
       ]
      }
     ],
     "prompt_number": 53
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Neste cen\u00e1rio, o problema que a rede neural deve resolver \u00e9 encontrar $L-1$ matrizes de par\u00e2metros, uma para cada camada, desde a primeira at\u00e9 a pen\u00faltima: $\\Theta^\\ell$. Suas dimens\u00f5es s\u00e3o: $U_{\\ell+1}\\times\\left(U_\\ell+1\\right)$.\n",
      "\n",
      "Assim, no nosso exemplo teremos 2 matrizes $\\Theta$:\n",
      "* $\\Theta^1 \\equiv []_{U_2\\times (U_1+ 1)} = []_{3\\times 3}$\n",
      "* $\\Theta^2 \\equiv []_{U_3\\times (U_2+1)} = []_{1\\times 4}$\n",
      "\n",
      "Ocorre que $\\Theta_1$ \u00e9 a matriz que, atuando sobre $x$, transforma-o num outro vetor, conforme essa transforma\u00e7\u00e3o:\n",
      "\n",
      "Chame $u_\\ell$ de a unidade $u$ da camada $\\ell$. Ent\u00e3o, tomemos, para come\u00e7ar, a unidade 1 da camada 2: $u_2 = 1$. Essa unidade transforma $x$ da seguinte maneira:\n",
      "$$\n",
      "z^2_1 = \\Theta^1_{10} + \\Theta^1_{11} x^1 + \\Theta^{1}_{12} x^2 = \\left[\\begin{matrix}\\Theta^1_{10} &\\Theta^1_{11} & \\Theta^{1}_{12}\\end{matrix}\\right] \\cdot \\left[\\begin{matrix}1\\\\x^1\\\\x^2\\\\\\end{matrix}\\right] = \\Theta^\\ell b^\\ell\n",
      "$$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Valor inicial de $\\Theta$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Ent\u00e3o, $\\Theta$ \u00e9 o conjunto de vari\u00e1veis cont\u00ednuas que, segundo nossa hip\u00f3tese (heur\u00edstica), \u00e9 capaz de estabelecer uma rela\u00e7\u00e3o entre $x$ e $y$ utilizando uma regress\u00e3o log\u00edstica linear (??).\n",
      "\n",
      "Criamos um vetor de matrizes $\\Theta$ de comprimento $L - 1$. Visando otimizar o funcionamento do algoritmo, precisamos criar essas matrizes com valores n\u00e3o nulos. Para isso, costuma-se popular as matrizes com valores aleat\u00f3rios, que variam um intervalo $[-\\epsilon_\\ell,+\\epsilon_\\ell]$, sendo que $\\epsilon_\\ell = \\sqrt{6/(u_{l} + u_{l+1})}$, com $\\ell \\in \\{0, 1, \\ldots, L-1\\}$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from numpy.random import random_sample\n",
      "import math\n",
      "\n",
      "thetas = []\n",
      "for l in np.arange(0,L-1):\n",
      "    epsilon = math.sqrt(6/(U[l]+U[l+1])) # Regra de ouro. N\u00e3o sei de onde vem :(\n",
      "    thetas.append(-epsilon + 2 * epsilon * random_sample((U[l+1],U[l] + 1))) # Matriz aleat\u00f3ria de dimens\u00f5es U_{l+1} por (U_l + 1)\n",
      "    \n",
      "for l in np.arange(0, L-1):\n",
      "    print \"Theta_\" + str(l) + \" =\\n\" + str(thetas[l])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Theta_0 =\n",
        "[[ 0.50346613 -0.05466565 -0.28574193]\n",
        " [-0.83087435  0.9928158   0.4998383 ]\n",
        " [-0.76309022 -0.31771149 -0.42150493]]\n",
        "Theta_1 =\n",
        "[[ 0.04489667 -0.39520445 -0.66300276  0.11190753]]\n"
       ]
      }
     ],
     "prompt_number": 54
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Esses s\u00e3o os par\u00e2metros que realizam a m\u00e1gica da rede neural.\n",
      "\n",
      "Deste modo, definimos uma posi\u00e7\u00e3o arbitr\u00e1ria no espa\u00e7o hiperdimensional dos par\u00e2metros $\\Theta$. A partir daqui, nosso objetivo \u00e9 percorrer a superf\u00edcie $J(\\Theta)$ procurando seu m\u00ednimo."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import operator\n",
      "n_params = reduce(operator.add, map(np.size, thetas))\n",
      "print \"Quantidade de par\u00e2metros que precisamos ajustar: \" + str(n_params)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Quantidade de par\u00e2metros que precisamos ajustar: 13\n"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Passo X: $X$ e $y$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Para este tutorial, vamos definir:\n",
      "\n",
      "$$\n",
      "f : \\mathcal{R}^2 \\to \\mathcal{R}\\\\\n",
      "y = f(x_1,x_2) = \\left(x_1 - 1/2\\right)^2 + (y - 1)^2\n",
      "$$\n",
      "\n",
      "Perceba que este \u00e9 um problema que se encaixa na rede neural $s$, que definimos anteriormente. Isto \u00e9, temos dois par\u00e2metros de entrada ($x_1$ e $x_2$) e um de sa\u00edda [$y \\equiv f(x_1,x_2)$]."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def f(x,y):\n",
      "    return (x - 0.5)**2 + (y - 1)**2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Mas a rede n\u00e3o conhece $f$. Tudo que ela receber\u00e1 \u00e9 um conjunto de pares $(x_1,x_2)$, cada qual associado a um $y$. Vamos criar esse conjunto por meio de uma malha de pontos no plano $x_1 \\times x_2$:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = np.transpose([[x,y] for x in range(-2,2) for y in range(-2,2)])\n",
      "print X"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[-2 -2 -2 -2 -1 -1 -1 -1  0  0  0  0  1  1  1  1]\n",
        " [-2 -1  0  1 -2 -1  0  1 -2 -1  0  1 -2 -1  0  1]]\n"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note que o conjunto de treinamento cont\u00e9m $M$ exemplos, isto \u00e9, temos $M$ ternos $(x_1,x_2,y)$ conhecidos. Usaremos eles para treinar a rede neural."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "M = X.shape[1]\n",
      "print \"M = \" + str(M)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "M = 16\n"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Agora, determinamos os $y$ associados:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def f(x,y):\n",
      "    return (x - 0.5)**2 + (y - 1)**2\n",
      "\n",
      "y = []\n",
      "for i in range(0,M):\n",
      "    x1, x2 = X[0,i], X[1,i]\n",
      "    y.append(f(x1,x2))\n",
      "    \n",
      "y = np.array(y)\n",
      "print y"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 15.25  10.25   7.25   6.25  11.25   6.25   3.25   2.25   9.25   4.25\n",
        "   1.25   0.25   9.25   4.25   1.25   0.25]\n"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Em tempo, perceba ainda que cada exemplo \u00e9 composto por um valor de entrada, composto por $N = 2 = U_1$ par\u00e2metros:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "N = X.shape[0]\n",
      "print \"N = \" + str(N)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "N = 2\n"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Al\u00e9m disso, cada valor de sa\u00edda \u00e9 composto por $U_L = 1$ par\u00e2metros (pois $y$ \u00e9 um vetor de n\u00fameros)."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "A fun\u00e7\u00e3o erro $J(\\Theta)$ e sua derivada $\\partial_\\Theta J$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "O algoritmo \"gradient descent\" (gradiente descendente) percorre a hipersuperf\u00edcie $J(\\Theta)$ procurando seu m\u00ednimo absoluto. Para isso, al\u00e9m do pr\u00f3prio valor de $J$, o algoritmo precisa da sua derivada $\\partial_\\Theta J$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "_Feedforward_"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\"Feedforward\" \u00e9 o procedimento para determinar o custo, ou erro, do ajuste caracterizado por $\\Theta$, ou seja, $J(\\Theta)$. Para isso, determine $a_\\ell$, $b_\\ell$ e $z_\\ell$ iterativamente, conforme as express\u00f5es abaixo:\n",
      "* $a^\\ell = \\begin{cases}X& \\text{se }\\ell = 1\\\\ g(z^\\ell) & \\text{se }\\ell > 1\\end{cases} \\equiv []_{U_\\ell\\times M}$\n",
      "* $b^\\ell = \\left[\\matrix{1\\\\a^\\ell}\\right] \\equiv []_{(U_\\ell+1)\\times M}$\n",
      "* $z^{\\ell+1} = \\Theta^{\\ell} b^{\\ell} \\equiv []_{U_\\ell\\times M}$\n",
      "\n",
      "Ap\u00f3s isso, determine $\\alpha = - Y \\odot \\log\\left[z^L\\right] - (1-Y) \\odot \\log\\left[1-z^L\\right] \\equiv []_{U_L\\times M}$ e, finalmente, $J(\\Theta) = \\frac{1}{M}\\text{sum}(\\alpha) + \\frac{\\lambda}{2M} \\textrm{sum}\\left(\\Theta_\\ell^2\\right)$.\n",
      "\n",
      "$\\odot$ \u00e9 o operador de multiplica\u00e7\u00e3o elemento-a-elemento e $\\text{sum}(\\alpha)$ soma todos os elementos de $\\alpha$. Note que, na express\u00e3o de $\\alpha$, $1$ \u00e9 a matriz identidade de dimens\u00f5es iguais \u00e0 de $Y$. $g(x) \\doteq 1/\\left(1 + e^{-x}\\right)$ \u00e9 a fun\u00e7\u00e3o \"sigmoid\"."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Come\u00e7ando com $\\ell = 0$, calculamos $a_0 = X$, uma matriz de dimens\u00f5es $U_0 \\times M$. Ela representa os par\u00e2metros que s\u00e3o inseridos na rede, isto \u00e9, que incidem na camada $\\ell = 0$. Em seguida, constru\u00edmos a matriz $b_0$, acrescentando uma linha de 1 no topo de $a_0$. Essa matriz, de dimens\u00e3o $(U_0 + 1)\\times M$, representa os par\u00e2metros que, emergindo da camada $\\ell = 0$, incidem na camada $\\ell = 1$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "_Back propagation_"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\"Back propagation\" \u00e9 o procedimento usado para determinar o gradiente de $J(\\Theta)$. Para isso, efetue os c\u00e1lculos abaixo iterativamente, da \u00faltima camada ($\\ell = L$) para a primeira ($\\ell = 1$)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* $\\delta^\\ell = \\begin{cases}z^L - Y & \\text{se } \\ell = L \\\\\\left(\\Xi^\\ell\\right)^T \\delta^{\\ell+1} \\odot g'(z^\\ell)& \\text{se }\\ell < L\\end{cases}$\n",
      "* $\\partial_{\\Theta_\\ell} J(\\Theta) = \\frac{1}{M}\\delta_{\\ell+1}\\left(b_\\ell\\right)^T + \\frac{\\lambda}{M}\\Xi_\\ell \\equiv []_{U_{\\ell+1}\\times(U_\\ell+1)}$\n",
      "\n",
      "para $\\ell = L, L-1, \\ldots, 2$ (n\u00e3o existe $\\delta^1$). $\\Xi^\\ell = \\Theta^\\ell(:,2:\\text{end})$ \u00e9 a pr\u00f3pria matriz $\\Theta^\\ell$, s\u00f3 que sem a primeira **coluna**."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Implementa\u00e7\u00e3o do feedforward e do back propagation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Abaixo, a implementa\u00e7\u00e3o da fun\u00e7\u00e3o $J(\\Theta, X, y, \\lambda)$, que retorna o par $(J, \\partial_{\\Theta} J)$. $\\lambda$ \u00e9 o par\u00e2metro de regulariza\u00e7\u00e3o, cuja fun\u00e7\u00e3o \u00e9 priorizar ajustes com par\u00e2metros de menor ordem."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def stack_ones(a):\n",
      "    ones = np.ones(1 if a.size == a.shape[0] else a.shape[1]) # TODO: tem um jeito melhor?\n",
      "    return np.vstack((ones, a))\n",
      "    \n",
      "def J(thetas, X, y, lamb):\n",
      "    zs = [1] # Esse 1 s\u00f3 serve para preencher\n",
      "    bs = []\n",
      "    reg = 0\n",
      "    \n",
      "    # Determina a fun\u00e7\u00e3o de custo do ajuste caracterizado pelos par\u00e2metros thetas\n",
      "    for layer in range(0, L-1):\n",
      "        a = X if layer == 0 else g(zs[layer])\n",
      "        b = stack_ones(a)\n",
      "        bs.append(b)\n",
      "        z = np.dot(thetas[layer], b)\n",
      "        zs.append(z)\n",
      "        reg = reg + np.power(thetas[layer][:,1:], 2).sum() # Regulariza\u00e7\u00e3o de J\n",
      "    \n",
      "    reg = reg * lamb / (2 * M)\n",
      "    Z = g(zs[L-1])\n",
      "    J = -(y * np.log(Z) + (1-y) * np.log(1-Z)).sum() / M + reg\n",
      "    \n",
      "    # Determina o gradiente de thetas\n",
      "    ds = [0 for _ in range(0,L)]\n",
      "    grad_J = [0 for _ in range(0,L-1)]\n",
      "    \n",
      "    for layer in range(L-1, -1, -1):\n",
      "        \n",
      "        if layer == L-1:\n",
      "            ds[layer] = zs[layer] - y\n",
      "        else:\n",
      "            xsi = np.delete(thetas[layer], 0, 1).T\n",
      "            ds[layer] = np.dot(xsi, ds[layer+1]) * grad_g(zs[layer])\n",
      "                \n",
      "        if layer < L-1:\n",
      "            grad_J[layer] = np.dot(ds[layer+1], bs[layer].T) / M\n",
      "            grad_J[layer][:,1:] = grad_J[layer][:,1:] + lamb/M * thetas[layer][:,1:] # Regulariza\u00e7\u00e3o de grad_J\n",
      "            \n",
      "    return (J, grad_J)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.optimize import fmin_bfgs\n",
      "\n",
      "def cost_function(theta):\n",
      "    cost, grad = J(theta, X, y, 0)\n",
      "    return cost\n",
      "\n",
      "def grad_cost(theta):\n",
      "    cost, grad = J(theta, X, y, 0)\n",
      "    return grad\n",
      "\n",
      "trained_thetas = fmin_bfgs(cost_function, thetas, maxiter=100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ValueError",
       "evalue": "operands could not be broadcast together with shapes (3,3) (1,4) ",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-43-29f4d8674575>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mtrained_thetas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfmin_bfgs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcost_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthetas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32m/usr/lib/python2.7/dist-packages/scipy/optimize/optimize.pyc\u001b[0m in \u001b[0;36mfmin_bfgs\u001b[1;34m(f, x0, fprime, args, gtol, norm, epsilon, maxiter, full_output, disp, retall, callback)\u001b[0m\n\u001b[0;32m    775\u001b[0m             'return_all': retall}\n\u001b[0;32m    776\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 777\u001b[1;33m     \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_minimize_bfgs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfprime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mopts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    778\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfull_output\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/lib/python2.7/dist-packages/scipy/optimize/optimize.pyc\u001b[0m in \u001b[0;36m_minimize_bfgs\u001b[1;34m(fun, x0, args, jac, callback, gtol, norm, eps, maxiter, disp, return_all, **unknown_options)\u001b[0m\n\u001b[0;32m    878\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# this was handled in numeric, let it remaines for more safety\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m             \u001b[0mrhok\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.0\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mZeroDivisionError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m             \u001b[0mrhok\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1000.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (3,3) (1,4) "
       ]
      }
     ],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trained_thetas"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 37,
       "text": [
        "array([ array([[-0.34167721,  0.24039915, -0.68358303],\n",
        "       [-0.4964911 ,  0.26256175, -0.33067524],\n",
        "       [ 0.40057997, -0.36521705,  0.36265823]]),\n",
        "       array([[ 11.2444133 ,  10.17116367,  10.39128689,  11.4790935 ]])], dtype=object)"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def predict(x, theta):\n",
      "    for layer in range(0, L-1):\n",
      "        a = x if layer == 0 else g(z)\n",
      "        z = np.dot(theta[layer], stack_ones(a))\n",
      "        \n",
      "    return g(z)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = np.array([[-2],\n",
      "              [-2]])\n",
      "y = predict(x, trained_thetas)\n",
      "y"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 39,
       "text": [
        "array([[ 1.]])"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 39
    }
   ],
   "metadata": {}
  }
 ]
}